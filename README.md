<div align="center">                                          
```
# A R I A - H E L I O S
### A Hyper-Extensible, Long-context Optimized Self-Regulating Transformer
  
[![Contact: Email]](mailto:emreaygul.work@gmail.com)

</div>

---

<p align="center">
  <strong><a href="#-tÃ¼rkÃ§e-versiyon">ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon</a></strong>
  Â Â Â â€¢Â Â Â 
  <strong><a href="#-english-version">ğŸ‡¬ğŸ‡§ English Version</a></strong>
</p>

---

<a name="tÃ¼rkÃ§e-versiyon"></a>
## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon

> **Not:** Bu depo, `ARIA-HELIOS` mimarisinin saÄŸlanan Python kaynak koduyla uyumlu teknik dÃ¶kÃ¼mantasyonunu sunmaktadÄ±r. Bu belge, modelin felsefesini, temel bileÅŸenlerini ve tasarÄ±m kararlarÄ±nÄ± detaylandÄ±rmak amacÄ±yla oluÅŸturulmuÅŸtur.

### ğŸ§  Teknik Felsefe: Otonom, Verimli ve Esnek Zeka

ARIA-Helios, gÃ¼nÃ¼mÃ¼zÃ¼n bÃ¼yÃ¼k dil modellerinin karÅŸÄ±laÅŸtÄ±ÄŸÄ± verimlilik ve Ã¶lÃ§eklenebilirlik zorluklarÄ±na yanÄ±t olarak tasarlanmÄ±ÅŸ, son teknoloji bir mimaridir. GeliÅŸtirmesi, Ã¼Ã§ temel ve birbiriyle sinerji iÃ§inde Ã§alÄ±ÅŸan ilkeye dayanmaktadÄ±r:

1.  **Kendi Kendini DÃ¼zenleyen Bilgi AkÄ±ÅŸÄ± (Self-Regulating Information Flow):** Standart `output = x + F(x)` artÄ±k baÄŸlantÄ±larÄ± yerine, Ã¶ÄŸrenilebilir **KapÄ±lÄ± Bellek Birimleri (Gated Memory Units - GMU)** kullanÄ±lÄ±r. Bu kapÄ±lar `output = (1 - gate) * x + gate * F(x)` formÃ¼lasyonu ile modelin hangi bilgiyi koruyacaÄŸÄ±na ve hangisini entegre edeceÄŸine dinamik olarak karar vermesini saÄŸlayarak daha stabil bir eÄŸitim ve daha zengin bir Ã¶zellik Ã¶ÄŸrenimi sunar.

2.  **Dinamik BaÄŸlam EsnekliÄŸi (Dynamic Context Flexibility):** Modelin anlama kapasitesi, eÄŸitim penceresiyle sÄ±nÄ±rlÄ± olmamalÄ±dÄ±r. **YaRN (Yet another RoPE extensioN method)**'dan ilham alan Ã¶lÃ§eklendirme, modelin yeniden eÄŸitime gerek kalmadan, eÄŸitimde gÃ¶rdÃ¼ÄŸÃ¼nÃ¼n Ã§ok Ã¶tesindeki dizi uzunluklarÄ±nÄ± etkili bir ÅŸekilde iÅŸlemesine olanak tanÄ±r.

3.  **Maksimum DonanÄ±m VerimliliÄŸi (Maximum Hardware Efficiency):** Mimarinin her katmanÄ±, modern hÄ±zlandÄ±rÄ±cÄ±lardan (GPU/TPU) maksimum verim almak ve VRAM kullanÄ±mÄ±nÄ± en aza indirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. **Gradient Checkpointing, CPU Offloading** ve **FP8 Activation Storage** gibi tekniklerin entegre kullanÄ±mÄ±, devasa modellerin daha eriÅŸilebilir donanÄ±mlarla eÄŸitilmesini mÃ¼mkÃ¼n kÄ±lar.

### ğŸ› ï¸ Mimari PlanÄ± ve Teknik BileÅŸenler

ARIA-Helios'un gÃ¼cÃ¼, birbiriyle uyum iÃ§inde Ã§alÄ±ÅŸan modern ve verimli bileÅŸenlerin birleÅŸiminden gelir.

*   #### Gated Memory Units (GMU)
    *   **Ne yapar?** Standart artÄ±k baÄŸlantÄ±larÄ±n yerini alÄ±r.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** Girdi (`x`) ve katman dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ (`F(x)`) Ã¶ÄŸrenilebilir bir sigmoid kapÄ±sÄ± ile dinamik olarak aÄŸÄ±rlÄ±klandÄ±rÄ±r. `bias = -3.0` ile baÅŸlatÄ±lan kapÄ±lar, modelin Ã¶nce stabil kimlik baÄŸlantÄ±larÄ±nÄ± Ã¶ÄŸrenmesini saÄŸlar.

*   #### Dynamic Context Scaling (YaRN-inspired)
    *   **Ne yapar?** Modelin eÄŸitimde gÃ¶rdÃ¼ÄŸÃ¼nden daha uzun metinleri iÅŸlemesini saÄŸlar.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** RoPE pozisyonel gÃ¶mmelerinin frekanslarÄ±nÄ± ve sorgu (query) vektÃ¶rlerinin genliÄŸini, baÄŸlam geniÅŸletme faktÃ¶rÃ¼ne gÃ¶re yeniden Ã¶lÃ§ekler.

*   #### Grouped-Query Attention (GQA)
    *   **Ne yapar?** Ã‡Ä±karÄ±m hÄ±zÄ±nÄ± artÄ±rÄ±r ve bellek bant geniÅŸliÄŸi ihtiyacÄ±nÄ± azaltÄ±r.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** Tam sayÄ±da Sorgu (Query) baÅŸlÄ±ÄŸÄ±na karÅŸÄ±lÄ±k daha az sayÄ±da Anahtar (Key) ve DeÄŸer (Value) baÅŸlÄ±ÄŸÄ± kullanarak KV-Cache boyutunu dramatik ÅŸekilde kÃ¼Ã§Ã¼ltÃ¼r.

*   #### Entegre Bellek ve Hesaplama OptimizasyonlarÄ±
    *   **Ne yapar?** SÄ±nÄ±rlÄ± VRAM ile Ã§ok bÃ¼yÃ¼k modellerin eÄŸitilmesini saÄŸlar.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** `Gradient Checkpointing`, `CPU Offload` ve `FP8 Activation Storage` gibi tekniklerin sinerjik kullanÄ±mÄ±yla GPU belleÄŸini maksimum verimlilikle yÃ¶netir.

### âœ¨ Temel FarklÄ±lÄ±klar ve GeliÅŸtirmeler

| Ã–zellik                 | Standart Transformer YaklaÅŸÄ±mÄ±                  | âœ… ARIA-HELIOS YaklaÅŸÄ±mÄ±                                                              |
| ------------------------ | ----------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Bilgi AkÄ±ÅŸÄ±**          | Statik ArtÄ±k BaÄŸlantÄ± (`y = x + F(x)`).          | **Dinamik GMU** (`y = (1-g)*x + g*F(x)`), Ã¶ÄŸrenilebilir bilgi akÄ±ÅŸ kontrolÃ¼.          |
| **BaÄŸlam Penceresi**     | EÄŸitildiÄŸi uzunlukla (Ã¶rn. 4K token) sÄ±nÄ±rlÄ±dÄ±r. | **Dinamik Ã–lÃ§ekleme (YaRN)** ile Ã§Ä±karÄ±mda baÄŸlamÄ± yeniden eÄŸitime gerek kalmadan geniÅŸletebilir. |
| **Dikkat MekanizmasÄ±**   | Multi-Head Attention (MHA).                     | **Grouped-Query Attention (GQA)** ile daha hÄ±zlÄ± Ã§Ä±karÄ±m ve dÃ¼ÅŸÃ¼k KV-cache boyutu.   |
| **DonanÄ±m VerimliliÄŸi**  | YÃ¼ksek VRAM tÃ¼ketimi, pahalÄ± donanÄ±m gerektirir. | **Checkpointing, Offload, FP8 Aktivasyonlar** ile entegre ve sinerjik bellek optimizasyonu. |
| **Ã‡Ä±karÄ±m HÄ±zÄ±**         | Standart PyTorch implementasyonlarÄ±.            | PyTorch 2.0+ **SDPA** (FlashAttention gibi) backend'lerini otomatik kullanarak maksimum hÄ±z. |

### ğŸš€ Durum ve Yol HaritasÄ±

*   **Mevcut SÃ¼rÃ¼m:** `H E L I O S (Hyper-Extensible Long-context Optimized System)`
*   **Durum:** âœ… Mimarinin referans implementasyonu tamamlandÄ±.
*   **GeliÅŸtirme:** â³ Aktif geliÅŸtirme ve kapalÄ± eÄŸitim altÄ±nda.
*   **Tarih:** `01.08.2025`

### ğŸ’¬ Topluluk ve Ä°letiÅŸim

Bu proje kapalÄ± kaynaklÄ± olsa da, yapay zeka topluluÄŸuyla etkileÅŸim kurmaktan ve fikir alÄ±ÅŸveriÅŸinde bulunmaktan heyecan duyuyoruz.

*   **ğŸ‘€ GeliÅŸmeleri Takip Etmek Ä°Ã§in:** Bu depoyu `Watch` ederek en son gÃ¼ncellemelerden haberdar olabilirsiniz.
*   **ğŸ¤ Ä°ÅŸbirliÄŸi ve EriÅŸim Talepleri Ä°Ã§in:** LÃ¼tfen [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com) Ã¼zerinden bizimle iletiÅŸime geÃ§in.
*   **ğŸŒ Topluluk:** YakÄ±nda duyurulacak Discord sunucumuz iÃ§in takipte kalÄ±n.

---
<br/>
<a name="english-version"></a>
## ğŸ‡¬ğŸ‡§ English Version

> **Note:** This repository provides the technical documentation for the `ARIA-HELIOS` architecture, consistent with the provided Python source code. This document is intended to detail the model's philosophy, core components, and design decisions.

### ğŸ§  Technical Philosophy: Autonomous, Efficient, and Flexible Intelligence

ARIA-Helios is a state-of-the-art architecture designed in response to the efficiency and scalability challenges faced by modern Large Language Models. Its development is based on three core, synergistic principles:

1.  **Self-Regulating Information Flow:** Instead of standard `output = x + F(x)` residual connections, it uses learnable **Gated Memory Units (GMU)**. These gates, following the `output = (1 - gate) * x + gate * F(x)` formulation, allow the model to dynamically decide what information to preserve and what to integrate, leading to more stable training and richer feature learning.

2.  **Dynamic Context Flexibility:** A model's comprehension should not be limited by its training window. Scaling inspired by **YaRN (Yet another RoPE extensioN method)** enables the model to effectively process sequence lengths far beyond its training context without requiring a full retrain.

3.  **Maximum Hardware Efficiency:** Every layer is designed to maximize throughput on modern accelerators (GPUs/TPUs) and minimize VRAM usage. The integrated use of techniques like **Gradient Checkpointing, CPU Offloading,** and **FP8 Activation Storage** makes it possible to train massive models on more accessible hardware.

### ğŸ› ï¸ Architectural Blueprint & Technical Components

The power of ARIA-Helios comes from the combination of modern, efficient components working in harmony.

*   #### Gated Memory Units (GMU)
    *   **What it does:** Replaces standard residual connections.
    *   **How it works:** Dynamically weights the input (`x`) and the layer's transformation (`F(x)`) using a learnable sigmoid gate. Initializing the gates with `bias = -3.0` allows the model to first learn stable identity connections.

*   #### Dynamic Context Scaling (YaRN-inspired)
    *   **What it does:** Enables the model to process texts longer than what it saw during training.
    *   **How it works:** It rescales the frequencies of RoPE positional embeddings and the magnitude of query vectors based on the context extension factor.

*   #### Grouped-Query Attention (GQA)
    *   **What it does:** Increases inference speed and reduces memory bandwidth requirements.
    *   **How it works:** It uses fewer Key/Value heads relative to Query heads, dramatically reducing the KV-Cache size.

*   #### Integrated Memory & Compute Optimizations
    *   **What it does:** Enables training of very large models on limited VRAM.
    *   **How it works:** Synergistically uses `Gradient Checkpointing`, `CPU Offload`, and `FP8 Activation Storage` to manage GPU memory with maximum efficiency.

### âœ¨ Core Differentiators & Enhancements

| Feature                  | Standard Transformer Approach                  | âœ… ARIA-HELIOS Approach                                                              |
| ------------------------ | ---------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Information Flow**     | Static Residual Connection (`y = x + F(x)`).   | **Dynamic GMU** (`y = (1-g)*x + g*F(x)`) for learnable information flow control. |
| **Context Window**       | Limited to its training length (e.g., 4K tokens).| Can extend context at inference time via **Dynamic Scaling (YaRN)** without retraining. |
| **Attention Mechanism**  | Multi-Head Attention (MHA).                    | **Grouped-Query Attention (GQA)** for faster inference and a smaller KV-cache.     |
| **Hardware Efficiency**  | High VRAM consumption, requires expensive hardware. | Integrated, synergistic memory optimization via **Checkpointing, Offload, & FP8 Activations**. |
| **Inference Speed**      | Standard PyTorch implementations.              | Automatically uses PyTorch 2.0+ **SDPA** backends (like FlashAttention) for max speed. |

### ğŸš€ Status & Roadmap

*   **Current Version:** `H E L I O S (Hyper-Extensible Long-context Optimized System)`
*   **Status:** âœ… Reference implementation of the architecture is complete.
*   **Development:** â³ Under active development and private training.
*   **Date:** `2025-08-01`

### ğŸ’¬ Community & Contact

While this project is closed-source, we are excited to engage with the AI community and exchange ideas.

*   **ğŸ‘€ To Follow Developments:** `Watch` this repository to be notified of the latest updates.
*   **ğŸ¤ For Collaboration and Access Inquiries:** Please contact us at [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com).
*   **ğŸŒ Community:** Stay tuned for our upcoming Discord server.

---

### ğŸ“œ Lisans (License)
> Copyright (c) 2025, ARIA Development Team
> 
> All rights reserved.
> 
> The materials contained in this repository, including but not limited to software, documentation, architectural descriptions, and all other content, are the proprietary property of the ARIA Development Team.
> 
> Without the prior express written permission of the ARIA Development Team, no part of these materials may be copied, reproduced, modified, distributed, reverse-engineered, or transmitted in any form or by any means.
> 
> Access to these materials is provided for informational and evaluation purposes only and does not grant any right of use, license, or ownership.
