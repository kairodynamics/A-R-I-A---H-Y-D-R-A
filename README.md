<div align="center">
  <h1 style="font-size: 3em; font-weight: bold; letter-spacing: 5px; text-shadow: 2px 2px 8px #666;">A R I A - P R O X I M A</h1>
  <h3 style="font-style: italic; color: #555;">A Progressive, Reflexive, Optimized, Hierarchical Intelligence & Memory Architecture</h3>
  <a href="mailto:emreaygul.work@gmail.com"><img alt="Contact: Email" src="https://img.shields.io/badge/Ä°letiÅŸim-E--posta-blue?style=flat-square&logo=gmail&logoColor=white"/></a>
  <a href="https://huggingface.co/kairodynedynamics/aria-helios/blob/main/LICENSE"><img alt="License" src="https://img.shields.io/badge/Lisans-Ã–zel%20(Tescilli)-red?style=flat-square"/></a>
  <a href="#"><img alt="Version" src="https://img.shields.io/badge/SÃ¼rÃ¼m-PROXIMA-purple?style=flat-square"/></a>
</div>

---

<p align="center">
  <strong><a href="#-tÃ¼rkÃ§e-versiyon">ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon</a></strong>
  Â Â Â â€¢Â Â Â 
  <strong><a href="#-english-version">ğŸ‡¬ğŸ‡§ English Version</a></strong>
</p>

---

<a name="tÃ¼rkÃ§e-versiyon"></a>
## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon

> **ARIA Projesi Evrimi:** Bu depo, ARIA projesinin en son ve en geliÅŸmiÅŸ sÃ¼rÃ¼mÃ¼ olan **`PROXIMA`** mimarisinin teknik vizyonunu ve kapalÄ± kaynak dokÃ¼mantasyonunu iÃ§ermektedir. `PROXIMA`, Ã¶nceki `HELIOS` prototipinin temel ilkelerini (dinamik bilgi akÄ±ÅŸÄ±, baÄŸlam esnekliÄŸi) alÄ±p, onlarÄ± hiyerarÅŸik zeka ve hibrit hesaplama paradigmalarÄ±yla birleÅŸtirerek yapay zeka mimarilerinde yeni bir Ã§Ä±ÄŸÄ±r aÃ§maktadÄ±r.

### ğŸ§  Teknik Felsefe: Adaptif, HiyerarÅŸik ve Hibrit Zeka

**ARIA-PROXIMA**, bÃ¼yÃ¼k dil modelleri evriminin bir sonraki adÄ±mÄ±dÄ±r. YoÄŸun (Llama), dÃ¼z uzmanlÄ± (Mixtral) veya salt durum-uzay (Mamba) mimarilerinin sunduÄŸu Ã§Ã¶zÃ¼mlerin Ã¶tesinde, bu paradigmalarÄ±n en gÃ¼Ã§lÃ¼ yÃ¶nlerini birleÅŸtiren ve zayÄ±flÄ±klarÄ±nÄ± ortadan kaldÄ±ran, **birleÅŸik bir zeka mimarisidir.**

`PROXIMA`, kaba kuvvet yerine zarafet, homojenlik yerine hiyerarÅŸi, statik akÄ±ÅŸ yerine dinamik kontrol sunar. Felsefesi dÃ¶rt temel ilke Ã¼zerine kuruludur:

1.  **AÅŸamalÄ± Bilgi Filtreleme (Progressive Information Filtering):** Standart artÄ±k baÄŸlantÄ±lar yerine, her iÅŸlem birimini saran **Proxima Gated Memory Cell (PGMC)** kullanÄ±lÄ±r. Bu Ã§ift kapÄ±lÄ± mekanizma, modelin her adÄ±mda *neyi dÃ¼ÅŸÃ¼neceÄŸini* ve *dÃ¼ÅŸÃ¼ncesini ne kadar gÃ¼ncelleyeceÄŸini* dinamik olarak belirlemesini saÄŸlar.

2.  **HiyerarÅŸik UzmanlaÅŸma (Hierarchical Specialization):** Parametreler, beyindeki kortikal hiyerarÅŸiyi taklit eden **HiyerarÅŸik Uzmanlar KarÄ±ÅŸÄ±mÄ± (H-MoE)** yapÄ±sÄ±yla organize edilir. Bu, devasa bir kapasitenin sÄ±ÄŸ bir havuzda deÄŸil, derin ve mantÄ±ksal bir dÃ¼zende Ã¶lÃ§eklenmesini saÄŸlar.

3.  **Hibrit BaÄŸlam Asimilasyonu (Hybrid Context Assimilation):** Model, tek bir baÄŸlam iÅŸleme yÃ¶nteminin sÄ±nÄ±rlamalarÄ±na takÄ±lÄ± kalmaz. KÄ±sa menzil iÃ§in **Reflective Attention** ve ultra uzun menzil iÃ§in **Long-Range State Assimilator (LRSA)** bloklarÄ±nÄ± birleÅŸtirerek hem anlÄ±k hassasiyet hem de sonsuz hafÄ±za potansiyeli sunar.

4.  **YansÄ±malÄ± Odaklanma (Reflective Focus):** Dikkat mekanizmasÄ±, anlamsal Ã¶neme gÃ¶re dikkat aÄŸÄ±rlÄ±klarÄ±nÄ± modÃ¼le eden Ã¶ÄŸrenilebilir bir "yansÄ±ma kapÄ±sÄ±" ile gÃ¼Ã§lendirilmiÅŸtir. Bu, modelin gÃ¼rÃ¼ltÃ¼yÃ¼ aktif olarak bastÄ±rmasÄ±nÄ± saÄŸlar.

---

### âš”ï¸ Rakip Mimarilere KarÅŸÄ± Stratejik ÃœstÃ¼nlÃ¼k: Neden PROXIMA?

`PROXIMA`, mevcut SOTA mimarilerin Ã§Ã¶zemediÄŸi temel sorunlara meydan okumak iÃ§in tasarlanmÄ±ÅŸtÄ±r.

| Meydan Okuma              | Geleneksel Ã‡Ã¶zÃ¼mler ve SÄ±nÄ±rlamalarÄ±                                                                                                | ğŸ‘‘ PROXIMA'nÄ±n ÃœstÃ¼n Ã‡Ã¶zÃ¼mÃ¼                                                                                                                                                                                                                                                          |
| ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Parametre VerimliliÄŸi** | **Llama (YoÄŸun):** Her token, tÃ¼m parametreleri aktive eder; maliyetli. <br/> **Mixtral (DÃ¼z MoE):** Verimli ama organizasyonsuz; uzmanlaÅŸma sÄ±ÄŸ kalabilir. | **HiyerarÅŸik MoE (H-MoE):** Hem verimli hem de organize. Bir "meta-yÃ¶nlendirici" Ã¶nce doÄŸru beyin lobunu, sonra doÄŸru nÃ¶ronu seÃ§er. Bu, daha derin ve anlamlÄ± bir uzmanlaÅŸma saÄŸlar. |
| **Bilgi AkÄ±ÅŸÄ± KontrolÃ¼**   | **TÃ¼m Transformer'lar:** KontrolsÃ¼z artÄ±k baÄŸlantÄ±lar (`x + F(x)`), gÃ¼rÃ¼ltÃ¼ birikimine ve eÄŸitimde kararsÄ±zlÄ±klara yol aÃ§ar.                                         | **Proxima Gated Memory Cell (PGMC):** Her katmanda bilgiyi bilinÃ§li olarak filtreler. Sadece gerekli bilgi iÅŸlenir, gereksiz olan bastÄ±rÄ±lÄ±r. Bu, daha temiz sinyal ve daha stabil Ã¶ÄŸrenme demektir.                      |
| **BaÄŸlam Ä°kilemi**        | **Llama/Mixtral (Salt Dikkat):** Karesel maliyet baÄŸlamÄ± sÄ±nÄ±rlar. <br/> **Mamba (Salt SSM):** Lineer Ã¶lÃ§eklenir ama yerel hassasiyeti dÃ¼ÅŸÃ¼k olabilir. | **Hibrit (Transformer + SSM):** Ä°ki dÃ¼nyanÄ±n en iyisi. YakÄ±n plan iÃ§in "mikroskop" (Reflective Attention), uzak plan iÃ§in "teleskop" (LRSA) kullanÄ±r. Tek mimaride hem hassasiyet hem de hafÄ±za.                  |
| **EÄŸitim Stratejisi**     | **Standart YaklaÅŸÄ±m:** TÃ¼m veriyi homojen olarak tekrar tekrar iÅŸlemek, kaynaklarÄ± verimsiz kullanÄ±r.                             | **DÃ¶ngÃ¼sel PekiÅŸtirme:** Ä°nsan Ã¶ÄŸrenmesini taklit eder. Ã–nce genel "keÅŸif", sonra en zorlu Ã¶rneklere odaklanan "yoÄŸunlaÅŸtÄ±rÄ±lmÄ±ÅŸ ustalÄ±k" fazÄ±. Verimlilik ve derinlik maksimize edilir. |

### ğŸš€ Durum ve Yol HaritasÄ±

*   **Mevcut SÃ¼rÃ¼m:** `P R O X I M A`
*   **Durum:** âœ… Referans implementasyon ve `v6.0` eÄŸitim stratejisi tamamlandÄ±.
*   **GeliÅŸtirme:** â³ Ã–zel veri kÃ¼meleri Ã¼zerinde aktif ve kapalÄ± devre eÄŸitim devam ediyor.
*   **Hedef Tarih:** `2026-Q1`

### ğŸ’¬ Topluluk ve Ä°letiÅŸim

Bu proje kapalÄ± kaynak kodlu olsa da, yapay zeka topluluÄŸu ile fikir alÄ±ÅŸveriÅŸinde bulunmaktan heyecan duyarÄ±z.

*   **ğŸ‘€ GeliÅŸmeleri Takip Etmek Ä°Ã§in:** Bu depoyu `Watch` butonuna tÄ±klayarak izleyebilirsiniz.
*   **ğŸ¤ Ä°ÅŸbirliÄŸi ve EriÅŸim Talepleri Ä°Ã§in:** LÃ¼tfen [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com) adresinden bizimle iletiÅŸime geÃ§in.
*   **ğŸŒ Topluluk:** YakÄ±nda duyurulacak Discord sunucumuz iÃ§in takipte kalÄ±n.

---
<br/>

<a name="english-version"></a>
## ğŸ‡¬ğŸ‡§ English Version

> **ARIA Project Evolution:** This repository contains the technical vision and closed-source documentation for **`PROXIMA`**, the latest and most advanced iteration of the ARIA project. `PROXIMA` elevates the core principles of its predecessor, the `HELIOS` prototype (dynamic information flow, context flexibility), by fusing them with hierarchical intelligence and hybrid computational paradigms, pioneering a new class of AI architectures.

### ğŸ§  Technical Philosophy: Adaptive, Hierarchical, and Hybrid Intelligence

**ARIA-PROXIMA** is the next step in the evolution of large language models. It moves beyond the solutions offered by dense (Llama), flat-expert (Mixtral), or pure state-space (Mamba) architectures to create a **unified intelligence architecture** that combines the greatest strengths of these paradigms while eliminating their weaknesses.

`PROXIMA` offers elegance over brute force, hierarchy over homogeneity, and dynamic control over static flow. Its philosophy is built on four core principles:

1.  **Progressive Information Filtering:** Instead of standard residual connections, **Proxima Gated Memory Cells (PGMC)** wrap each processing unit. This dual-gate mechanism allows the model to dynamically decide *what to think about* and *how much to update its state*, ensuring a focused and noise-resistant learning process.

2.  **Hierarchical Specialization:** Parameters are organized within a **Hierarchical Mixture-of-Experts (H-MoE)** structure that mimics the brain's cortical hierarchy, ensuring that massive capacity is scaled in a deep and logical manner, not a shallow pool.

3.  **Hybrid Context Assimilation:** The model is not confined by a single context-processing method. It combines **Reflective Attention** for short-range tasks and **Long-Range State Assimilators (LRSA)** for ultra-long sequences, offering both high-fidelity understanding and a virtually infinite memory span.

4.  **Reflective Focus:** The attention mechanism is enhanced with a learnable "reflection gate" that modulates attention weights based on semantic importance, allowing the model to actively suppress noise.

---

### âš”ï¸ Strategic Superiority Over Competing Architectures: Why PROXIMA?

`PROXIMA` is designed to challenge the fundamental limitations of current SOTA architectures.

| Challenge                 | Conventional Solutions & Their Limitations                                                                                             | ğŸ‘‘ PROXIMA's Superior Solution                                                                                                                                                                                                                                                           |
| ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Parameter Efficiency**  | **Llama (Dense):** Every token activates all parameters; computationally expensive. <br/> **Mixtral (Flat MoE):** Efficient, but disorganized; specialization can remain shallow. | **Hierarchical MoE (H-MoE):** Both efficient and organized. A "meta-router" first selects the right brain lobe (expert group), then the right neuron (expert), enabling deeper and more meaningful specialization. |
| **Information Flow Control** | **All Transformers:** Uncontrolled residual connections (`x + F(x)`) lead to noise accumulation and training instability.                                         | **Proxima Gated Memory Cell (PGMC):** Consciously filters information at every layer. Only necessary information is processed, and irrelevant data is suppressed, leading to a cleaner signal and more stable learning.                      |
| **The Context Dilemma**   | **Llama/Mixtral (Attention-Only):** Quadratic cost limits practical context. <br/> **Mamba (SSM-Only):** Scales linearly but may lack the high-fidelity local understanding of transformers. | **Hybrid (Transformer + SSM):** The best of both worlds. It uses a "microscope" (Reflective Attention) for the close-up view and a "telescope" (LRSA) for the long view, offering both precision and memory in a single architecture.                  |
| **Training Strategy**     | **Standard Approach:** Repeatedly processing the entire dataset, inefficiently using resources on already-learned examples.                             | **Cyclic Consolidation:** Mimics human learning. First, a broad "discovery" phase, followed by a "focused mastery" phase on the most challenging, information-dense samples, maximizing both efficiency and depth. |

### ğŸš€ Status and Roadmap

*   **Current Version:** `P R O X I M A`
*   **Status:** âœ… Reference implementation and `v6.0` training strategy are complete.
*   **Development:** â³ Under active, closed-door training on proprietary datasets.
*   **Target Date:** `Q1 2026`

### ğŸ’¬ Community and Contact

Although this project is closed-source, we are excited to engage with the AI community and exchange ideas.

*   **ğŸ‘€ To Follow Developments:** You can stay informed about the latest updates by `Watch`ing this repository.
*   **ğŸ¤ For Collaboration and Access Requests:** Please contact us via [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com).
*   **ğŸŒ Community:** Stay tuned for our Discord server, to be announced soon.

---

### ğŸ“œ License

The architecture, software, and all materials in this project are protected under a **special, proprietary license** that governs non-commercial, evaluation-only use. The full text of the license details the core principles and usage restrictions of the project.

> **[Click Here to Read the Full License](https://huggingface.co/kairodynedynamics/aria-helios/blob/main/LICENSE)**
