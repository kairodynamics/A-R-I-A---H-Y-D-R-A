<div align="center">                                          

# A R I A - H E L I O S
### A Hyper-Extensible, Long-context Optimized Self-Regulating Transformer
  
[![Contact: Email]](mailto:emreaygul.work@gmail.com)

</div>

---

<p align="center">
  <strong><a href="#-tÃ¼rkÃ§e-versiyon">ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon</a></strong>
  Â Â Â â€¢Â Â Â 
  <strong><a href="#-english-version">ğŸ‡¬ğŸ‡§ English Version</a></strong>
</p>

---

<a name="tÃ¼rkÃ§e-versiyon"></a>
## ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e Versiyon

> **Not:** Bu depo, `ARIA-HELIOS` mimarisinin kapalÄ± teknik dÃ¶kÃ¼mantasyonunu sunmaktadÄ±r. Bu belge, modelin felsefesini, temel bileÅŸenlerini ve tasarÄ±m kararlarÄ±nÄ± detaylandÄ±rmak amacÄ±yla oluÅŸturulmuÅŸtur.

### ğŸ§  Teknik Felsefe: Otonom, Verimli ve Esnek Zeka

ARIA-Helios, gÃ¼nÃ¼mÃ¼zÃ¼n bÃ¼yÃ¼k dil modellerinin karÅŸÄ±laÅŸtÄ±ÄŸÄ± verimlilik ve Ã¶lÃ§eklenebilirlik zorluklarÄ±na yanÄ±t olarak tasarlanmÄ±ÅŸ, son teknoloji bir mimaridir. GeliÅŸtirmesi, Ã¼Ã§ temel ve birbiriyle sinerji iÃ§inde Ã§alÄ±ÅŸan ilkeye dayanmaktadÄ±r:

1.  **Kendi Kendini DÃ¼zenleyen Bilgi AkÄ±ÅŸÄ± (Self-Regulating Information Flow):** Standart artÄ±k baÄŸlantÄ±larÄ± yerine, Ã¶ÄŸrenilebilir **KapÄ±lÄ± Bellek Birimleri (Gated Memory Units - GMU)** kullanÄ±lÄ±r. Bu kapÄ±lar modelin hangi bilgiyi koruyacaÄŸÄ±na ve hangisini entegre edeceÄŸine dinamik olarak karar vermesini saÄŸlayarak daha stabil bir eÄŸitim ve daha zengin bir Ã¶zellik Ã¶ÄŸrenimi sunar.

2.  **Dinamik BaÄŸlam EsnekliÄŸi (Dynamic Context Flexibility):** Modelin anlama kapasitesi, eÄŸitim penceresiyle sÄ±nÄ±rlÄ± olmamalÄ±dÄ±r. **YaRN (Yet another RoPE extensioN method)**'dan ilham alan Ã¶lÃ§eklendirme, modelin yeniden eÄŸitime gerek kalmadan, eÄŸitimde gÃ¶rdÃ¼ÄŸÃ¼nÃ¼n Ã§ok Ã¶tesindeki dizi uzunluklarÄ±nÄ± etkili bir ÅŸekilde iÅŸlemesine olanak tanÄ±r.

3.  **Maksimum DonanÄ±m VerimliliÄŸi (Maximum Hardware Efficiency):** Mimarinin her katmanÄ±, modern hÄ±zlandÄ±rÄ±cÄ±lardan (GPU/TPU) maksimum verim almak ve VRAM kullanÄ±mÄ±nÄ± en aza indirmek iÃ§in tasarlanmÄ±ÅŸtÄ±r. **Gradient Checkpointing, CPU Offloading** ve **FP8 Activation Storage** gibi tekniklerin entegre kullanÄ±mÄ±, devasa modellerin daha eriÅŸilebilir donanÄ±mlarla eÄŸitilmesini mÃ¼mkÃ¼n kÄ±lar.

### ğŸ› ï¸ Mimari PlanÄ± ve Teknik BileÅŸenler

ARIA-Helios'un gÃ¼cÃ¼, birbiriyle uyum iÃ§inde Ã§alÄ±ÅŸan modern ve verimli bileÅŸenlerin birleÅŸiminden gelir.

*   #### Gated Memory Units (GMU)
    *   **Ne yapar?** Standart artÄ±k baÄŸlantÄ±larÄ±n yerini alÄ±r.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** Girdi ve katman dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ Ã¶ÄŸrenilebilir bir sigmoid kapÄ±sÄ± ile dinamik olarak aÄŸÄ±rlÄ±klandÄ±rÄ±r. Eksi deÄŸer ile baÅŸlatÄ±lan kapÄ±lar, modelin Ã¶nce stabil kimlik baÄŸlantÄ±larÄ±nÄ± Ã¶ÄŸrenmesini saÄŸlar.

*   #### Dynamic Context Scaling (YaRN-inspired)
    *   **Ne yapar?** Modelin eÄŸitimde gÃ¶rdÃ¼ÄŸÃ¼nden daha uzun metinleri iÅŸlemesini saÄŸlar.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** RoPE pozisyonel gÃ¶mmelerinin frekanslarÄ±nÄ± ve sorgu (query) vektÃ¶rlerinin genliÄŸini, baÄŸlam geniÅŸletme faktÃ¶rÃ¼ne gÃ¶re yeniden Ã¶lÃ§ekler.

*   #### Grouped-Query Attention (GQA)
    *   **Ne yapar?** Ã‡Ä±karÄ±m hÄ±zÄ±nÄ± artÄ±rÄ±r ve bellek bant geniÅŸliÄŸi ihtiyacÄ±nÄ± azaltÄ±r.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** Tam sayÄ±da Sorgu (Query) baÅŸlÄ±ÄŸÄ±na karÅŸÄ±lÄ±k daha az sayÄ±da Anahtar (Key) ve DeÄŸer (Value) baÅŸlÄ±ÄŸÄ± kullanarak KV-Cache boyutunu dramatik ÅŸekilde kÃ¼Ã§Ã¼ltÃ¼r.

*   #### Entegre Bellek ve Hesaplama OptimizasyonlarÄ±
    *   **Ne yapar?** SÄ±nÄ±rlÄ± VRAM ile Ã§ok bÃ¼yÃ¼k modellerin eÄŸitilmesini saÄŸlar.
    *   **NasÄ±l Ã§alÄ±ÅŸÄ±r?** `Gradient Checkpointing`, `CPU Offload` ve `FP8 Activation Storage` gibi tekniklerin sinerjik kullanÄ±mÄ±yla GPU belleÄŸini maksimum verimlilikle yÃ¶netir.

### âœ¨ Temel FarklÄ±lÄ±klar ve GeliÅŸtirmeler

| Ã–zellik                 | Standart Transformer YaklaÅŸÄ±mÄ±                  | âœ… ARIA-HELIOS YaklaÅŸÄ±mÄ±                                                              |
| ------------------------ | ----------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Bilgi AkÄ±ÅŸÄ±**          | Statik ArtÄ±k BaÄŸlantÄ±.          | **Dinamik GMU** ile Ã¶ÄŸrenilebilir bilgi akÄ±ÅŸ kontrolÃ¼.          |
| **BaÄŸlam Penceresi**     | EÄŸitildiÄŸi uzunlukla (Ã¶rn. 4K token) sÄ±nÄ±rlÄ±dÄ±r. | **Dinamik Ã–lÃ§ekleme (YaRN)** ile Ã§Ä±karÄ±mda baÄŸlamÄ± yeniden eÄŸitime gerek kalmadan geniÅŸletebilir. |
| **Dikkat MekanizmasÄ±**   | Multi-Head Attention (MHA).                     | **Grouped-Query Attention (GQA)** ile daha hÄ±zlÄ± Ã§Ä±karÄ±m ve dÃ¼ÅŸÃ¼k KV-cache boyutu.   |
| **DonanÄ±m VerimliliÄŸi**  | YÃ¼ksek VRAM tÃ¼ketimi, pahalÄ± donanÄ±m gerektirir. | **Checkpointing, Offload, FP8 Aktivasyonlar** ile entegre ve sinerjik bellek optimizasyonu. |
| **Ã‡Ä±karÄ±m HÄ±zÄ±**         | Standart PyTorch implementasyonlarÄ±.            | PyTorch 2.0+ **SDPA** (FlashAttention gibi) backend'lerini otomatik kullanarak maksimum hÄ±z. |

### ğŸš€ Durum ve Yol HaritasÄ±

*   **Mevcut SÃ¼rÃ¼m:** `H E L I O S (Hyper-Extensible Long-context Optimized System)`
*   **Durum:** âœ… Mimarinin referans implementasyonu tamamlandÄ±.
*   **GeliÅŸtirme:** â³ Aktif geliÅŸtirme ve kapalÄ± eÄŸitim altÄ±nda.
*   **Tarih:** `01.08.2025`

### ğŸ’¬ Topluluk ve Ä°letiÅŸim

Bu proje kapalÄ± kaynaklÄ± olsa da, yapay zeka topluluÄŸuyla etkileÅŸim kurmaktan ve fikir alÄ±ÅŸveriÅŸinde bulunmaktan heyecan duyuyoruz.

*   **ğŸ‘€ GeliÅŸmeleri Takip Etmek Ä°Ã§in:** Bu depoyu `Watch` ederek en son gÃ¼ncellemelerden haberdar olabilirsiniz.
*   **ğŸ¤ Ä°ÅŸbirliÄŸi ve EriÅŸim Talepleri Ä°Ã§in:** LÃ¼tfen [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com) Ã¼zerinden bizimle iletiÅŸime geÃ§in.
*   **ğŸŒ Topluluk:** YakÄ±nda duyurulacak Discord sunucumuz iÃ§in takipte kalÄ±n.

---
<br/>

<a name="english-version"></a>
## ğŸ‡¬ğŸ‡§ English Version

> **Note:** This repository provides the technical documentation for the `ARIA-HELIOS` architecture, compatible with the provided Python source code. This document was created to detail the model's philosophy, core components, and design decisions.

### ğŸ§  Technical Philosophy: Autonomous, Efficient, and Flexible Intelligence

ARIA-Helios is a state-of-the-art architecture designed in response to the efficiency and scalability challenges faced by today's large language models. Its development is based on three fundamental and synergistic principles:

1.  **Self-Regulating Information Flow:** Instead of standard residual connections, learnable **Gated Memory Units (GMUs)** are used. These gates allow the model to dynamically decide which information to preserve and which to integrate, leading to more stable training and richer feature learning.

2.  **Dynamic Context Flexibility:** The model's comprehension capacity should not be limited by its training window. Scaling inspired by **YaRN (Yet another RoPE extensioN method)** allows the model to effectively handle sequence lengths far beyond what it has seen during training, without the need for retraining.

3.  **Maximum Hardware Efficiency:** Every layer of the architecture is designed to maximize throughput on modern accelerators (GPU/TPU) and minimize VRAM usage. The integrated use of techniques like **Gradient Checkpointing, CPU Offloading**, and **FP8 Activation Storage** makes it possible to train massive models on more accessible hardware.

### ğŸ› ï¸ Architecture Blueprint and Technical Components

The power of ARIA-Helios comes from the combination of modern and efficient components working in harmony.

*   #### Gated Memory Units (GMU)
    *   **What it does:** Replaces standard residual connections.
    *   **How it works:** It dynamically weights the input and the layer transformation with a learnable sigmoid gate. The gates are initialized with a negative bias, encouraging the model to first learn stable identity connections.

*   #### Dynamic Context Scaling (YaRN-inspired)
    *   **What it does:** Enables the model to process texts longer than those seen during training.
    *   **How it works:** It rescales the frequencies of RoPE positional embeddings and the magnitude of query vectors according to the context extension factor.

*   #### Grouped-Query Attention (GQA)
    *   **What it does:** Increases inference speed and reduces memory bandwidth requirements.
    *   **How it works:** It dramatically reduces the KV-Cache size by using fewer Key (K) and Value (V) heads than the full number of Query (Q) heads.

*   #### Integrated Memory and Computation Optimizations
    *   **What it does:** Enables the training of very large models with limited VRAM.
    *   **How it works:** It manages GPU memory with maximum efficiency through the synergistic use of techniques like `Gradient Checkpointing`, `CPU Offload`, and `FP8 Activation Storage`.

### âœ¨ Key Differences and Improvements

| Feature                  | Standard Transformer Approach                   | âœ… ARIA-HELIOS Approach                                                              |
| ------------------------ | ----------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Information Flow**     | Static Residual Connection                      | Learnable information flow control with **Dynamic GMU**.                           |
| **Context Window**       | Limited to training length (e.g., 4K tokens).   | Can extend context at inference time without retraining via **Dynamic Scaling (YaRN)**. |
| **Attention Mechanism**  | Multi-Head Attention (MHA).                     | Faster inference and smaller KV-cache with **Grouped-Query Attention (GQA)**.      |
| **Hardware Efficiency**  | High VRAM consumption, requires expensive hardware. | Integrated and synergistic memory optimization with **Checkpointing, Offload, FP8 Activations**. |
| **Inference Speed**      | Standard PyTorch implementations.               | Maximum speed by automatically using PyTorch 2.0+ **SDPA** backends (like FlashAttention). |

### ğŸš€ Status and Roadmap

*   **Current Version:** `H E L I O S (Hyper-Extensible Long-context Optimized System)`
*   **Status:** âœ… Reference implementation of the architecture is complete.
*   **Development:** â³ Under active development and closed training.
*   **Date:** `01.08.2025`

### ğŸ’¬ Community and Contact

Although this project is closed-source, we are excited to engage with the AI community and exchange ideas.

*   **ğŸ‘€ To Follow Developments:** You can stay informed about the latest updates by `Watch`ing this repository.
*   **ğŸ¤ For Collaboration and Access Requests:** Please contact us via [`emreaygul.work@gmail.com`](mailto:emreaygul.work@gmail.com).
*   **ğŸŒ Community:** Stay tuned for our Discord server, to be announced soon.

---

### ğŸ“œ Lisans (License)
> Copyright (c) 2025, ARIA Development Team
> 
> All rights reserved.
> 
> The materials contained in this repository, including but not limited to software, documentation, architectural descriptions, and all other content, are the proprietary property of the ARIA Development Team.
> 
> Without the prior express written permission of the ARIA Development Team, no part of these materials may be copied, reproduced, modified, distributed, reverse-engineered, or transmitted in any form or by any means.
> 
> Access to these materials is provided for informational and evaluation purposes only and does not grant any right of use, license, or ownership.
